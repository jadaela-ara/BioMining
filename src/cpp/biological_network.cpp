#include <cmath>
#include "bio/biological_network.h"
#include <QDebug>
#include <QMutexLocker>
#include <QRandomGenerator>
#include <QJsonDocument>
#include <QFile>
#include <QStandardPaths>
#include <QDir>
#include <QCryptographicHash>
#include <QtMath>
#include <algorithm>
#include <numeric>
#include <thread>
#include <atomic>
#include <QDataStream>

using namespace BioMining::Network; 

BiologicalNetwork::BiologicalNetwork(QObject *parent)
    : QObject(parent)
    , m_learningState(LearningState::Untrained)
    , m_currentEpoch(0)
    , m_totalEpochs(0)
    , m_trainingProgress(0.0)
    , m_learningTimer(std::make_unique<QTimer>(this))
    , m_optimizationTimer(std::make_unique<QTimer>(this))
    , m_networkEfficiency(0.0)
    , m_learningActive(false)
    , m_successfulPredictions(0)
    , m_totalPredictions(0)
    , m_averageConfidence(0.0)
{
    // Configuration par d√©faut
    m_config = NetworkConfig();
    
    // Timers
    connect(m_learningTimer.get(), &QTimer::timeout, this, &BiologicalNetwork::onLearningCycle);
    connect(m_optimizationTimer.get(), &QTimer::timeout, this, &BiologicalNetwork::onOptimizationTimer);
    
    // Initialisation du r√©seau
    initializeNetwork();
    
    qDebug() << "[BIO-NET] R√©seau biologique initialis√© avec" << m_config.neuronCount << "neurones";
}

BiologicalNetwork::~BiologicalNetwork()
{
    stopLearning();
}

void BiologicalNetwork::setNetworkConfig(const NetworkConfig &config)
{
    QMutexLocker locker(&m_networkMutex);
    
    m_config = config;
    
    // R√©initialisation si la topologie change
    if (m_layers.isEmpty() || m_layers[0].neurons.size() != config.neuronCount) {
        initializeNetwork();
    }
    
    qDebug() << "[BIO-NET] Configuration mise √† jour - Neurones:" << config.neuronCount
             << "Couches:" << config.hiddenLayers
             << "Taux apprentissage:" << config.learningRate;
}

BiologicalNetwork::NetworkConfig BiologicalNetwork::getNetworkConfig() const
{
    QMutexLocker locker(&m_networkMutex);
    return m_config;
}

BiologicalNetwork::LearningState BiologicalNetwork::getLearningState() const
{
    QMutexLocker locker(&m_networkMutex);
    return m_learningState;
}

double BiologicalNetwork::getTrainingProgress() const
{
    QMutexLocker locker(&m_networkMutex);
    return m_trainingProgress;
}

int BiologicalNetwork::getTrainingEpochs() const
{
    QMutexLocker locker(&m_networkMutex);
    return m_currentEpoch;
}

void BiologicalNetwork::initializeNetwork()
{
    m_layers.clear();
    
    // Couche d'entr√©e (signaux MEA)
    NetworkLayer inputLayer;
    inputLayer.layerType = "input";
    inputLayer.neurons.resize(m_config.neuronCount);
    
    for (int i = 0; i < m_config.neuronCount; ++i) {
        BiologicalNeuron &neuron = inputLayer.neurons[i];
        neuron.activation = 0.0;
        neuron.threshold = DEFAULT_ACTIVATION_THRESHOLD + (QRandomGenerator::global()->generateDouble() - 0.5) * 0.2;
        neuron.lastStimulation = 0.0;
        neuron.isActive = true;
        neuron.connectionCount = 0;
        neuron.adaptationFactor = 1.0;
        neuron.bitcoin_response_score = 0.0;  // ‚Üê AJOUT
    }
    
    m_layers.append(inputLayer);
    
    // Couches cach√©es (traitement biologique)
    for (int layer = 0; layer < m_config.hiddenLayers.size(); ++layer) {
        NetworkLayer hiddenLayer;
        hiddenLayer.layerType = "hidden";
        
        int layerSize = m_config.neuronCount / (layer + 2); // R√©duction progressive
        layerSize = qMax(layerSize, 10); // Minimum 10 neurones par couche
        
        hiddenLayer.neurons.resize(layerSize);
        
        for (int i = 0; i < layerSize; ++i) {
            BiologicalNeuron &neuron = hiddenLayer.neurons[i];
            neuron.activation = 0.0;
            neuron.threshold = DEFAULT_ACTIVATION_THRESHOLD + (QRandomGenerator::global()->generateDouble() - 0.5) * 0.3;
            neuron.lastStimulation = 0.0;
            neuron.isActive = true;
            neuron.connectionCount = 0;
            neuron.adaptationFactor = 1.0;
            neuron.bitcoin_response_score = 0.0;  // ‚Üê AJOUT
            
            // Connexions avec la couche pr√©c√©dente
            int prevLayerSize = m_layers.last().neurons.size();
            neuron.weights.resize(prevLayerSize);
            neuron.dendrites.resize(prevLayerSize);
            
            // Initialisation des poids (distribution normale)
            for (int w = 0; w < prevLayerSize; ++w) {
                neuron.weights[w] = (QRandomGenerator::global()->generateDouble() - 0.5) * 2.0 / sqrt(prevLayerSize);
                neuron.dendrites[w] = 0.0;
            }
            
            neuron.connectionCount = prevLayerSize;
        }
        
        m_layers.append(hiddenLayer);
    }
    
    // Couche de sortie (pr√©diction de nonce)
    NetworkLayer outputLayer;
    outputLayer.layerType = "output";
    outputLayer.neurons.resize(32); // 32 bits pour nonce suggestions
    
    for (int i = 0; i < 32; ++i) {
        BiologicalNeuron &neuron = outputLayer.neurons[i];
        neuron.activation = 0.0;
        neuron.threshold = DEFAULT_ACTIVATION_THRESHOLD;
        neuron.lastStimulation = 0.0;
        neuron.isActive = true;
        neuron.connectionCount = 0;
        neuron.adaptationFactor = 1.0;
        neuron.bitcoin_response_score = 0.0;  // ‚Üê AJOUT
        
        // Connexions avec la derni√®re couche cach√©e
        int prevLayerSize = m_layers.last().neurons.size();
        neuron.weights.resize(prevLayerSize);
        neuron.dendrites.resize(prevLayerSize);
        
        for (int w = 0; w < prevLayerSize; ++w) {
            neuron.weights[w] = (QRandomGenerator::global()->generateDouble() - 0.5) * 2.0 / sqrt(prevLayerSize);
            neuron.dendrites[w] = 0.0;
        }
        
        neuron.connectionCount = prevLayerSize;
    }
    
    m_layers.append(outputLayer);
    
    createNetworkTopology();
    
    qDebug() << "[BIO-NET] R√©seau initialis√© avec" << m_layers.size() << "couches";
    for (int i = 0; i < m_layers.size(); ++i) {
        qDebug() << "  Couche" << i << "(" << m_layers[i].layerType << "):" << m_layers[i].neurons.size() << "neurones";
    }
}

void BiologicalNetwork::createNetworkTopology()
{
    // √âtablissement des connexions synaptiques inter-couches
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &currentLayer = m_layers[layerIdx];
        const NetworkLayer &prevLayer = m_layers[layerIdx - 1];
        
        // Matrice de connexions synaptiques
        currentLayer.synapses.resize(currentLayer.neurons.size());
        
        for (int neuronIdx = 0; neuronIdx < currentLayer.neurons.size(); ++neuronIdx) {
            currentLayer.synapses[neuronIdx].resize(prevLayer.neurons.size());
            
            for (int prevNeuronIdx = 0; prevNeuronIdx < prevLayer.neurons.size(); ++prevNeuronIdx) {
                // Force synaptique initiale bas√©e sur proximit√© spatiale (simulation MEA)
                double distance = abs(neuronIdx - (prevNeuronIdx * currentLayer.neurons.size() / prevLayer.neurons.size()));
                double maxDistance = currentLayer.neurons.size();
                double proximityFactor = 1.0 - (distance / maxDistance);
                
                // Connexion synaptique avec variabilit√© biologique
                double synapticStrength = proximityFactor * (0.5 + QRandomGenerator::global()->generateDouble() * 0.5);
                currentLayer.synapses[neuronIdx][prevNeuronIdx] = synapticStrength;
            }
        }
    }
}

bool BiologicalNetwork::startInitialLearning(int trainingCycles)
{
    QMutexLocker locker(&m_networkMutex);
    
    if (m_learningState != LearningState::Untrained) {
        qWarning() << "[BIO-NET] Apprentissage d√©j√† en cours ou termin√©";
        return false;
    }
    
    m_learningState = LearningState::InitialLearning;
    m_currentEpoch = 0;
    m_totalEpochs = trainingCycles;
    m_trainingProgress = 0.0;
    
    // G√©n√©ration de donn√©es d'apprentissage synth√©tiques
    generateTrainingData();
    
    // D√©marrage des cycles d'apprentissage
    //m_learningTimer->start(50); // 50ms entre cycles = 20 Hz

    // D√©marrage du thread d'apprentissage
    m_learningActive = true;
    if (m_learningThread.joinable()) {
        m_learningThread.join();
    }
    m_learningThread = std::thread([this]() {
        while (m_learningActive && m_currentEpoch <= m_totalEpochs) {
            this->onLearningCycle();
            std::this_thread::sleep_for(std::chrono::milliseconds(50));
        }
        // Fin de l'apprentissage
        //if (m_learningActive) {
        //    this->stopLearning();
        //}
    });
    
    qDebug() << "[BIO-NET] Apprentissage initial d√©marr√© -" << trainingCycles << "cycles";
    emit learningStateChanged(m_learningState);
    
    return true;
}

/* VERSION INIT
void BiologicalNetwork::generateTrainingData()
{
    // G√©n√©ration de patterns d'apprentissage pour l'initialisation
    
    for (int i = 0; i < 1000; ++i) {
        LearningData trainingExample;
        
        // Signaux MEA simul√©s avec patterns biologiques r√©alistes
        trainingExample.inputSignals.resize(m_config.neuronCount);
        
        for (int electrode = 0; electrode < m_config.neuronCount; ++electrode) {
            // Pattern neural avec composantes fr√©quentielles
            double neuralBase = sin(i * 0.1 + electrode * 0.05) * 0.3;
            double neuralSpike = (QRandomGenerator::global()->generateDouble() < 0.1) ? 
                                QRandomGenerator::global()->generateDouble() * 2.0 : 0.0;
            double noise = (QRandomGenerator::global()->generateDouble() - 0.5) * 0.1;
            
            trainingExample.inputSignals[electrode] = neuralBase + neuralSpike + noise;
        }
        
        // G√©n√©ration de nonce cible bas√© sur pattern
        double patternSum = std::accumulate(trainingExample.inputSignals.begin(), 
                                          trainingExample.inputSignals.end(), 0.0);
        
        trainingExample.targetNonce = static_cast<uint64_t>(abs(patternSum * 1e6)) % (1ULL << 32);
        trainingExample.blockHeader = QString("training_block_%1").arg(i);
        trainingExample.difficulty = 0x0000FFFFFFFFFFFF >> (i % 4); // Difficult√© variable
        trainingExample.wasSuccessful = (i % 3 == 0); // 33% de succ√®s simul√©
        trainingExample.attempts = 1000 + (i % 5000);
        trainingExample.computeTime = 1.0 + (i % 10);
        trainingExample.timestamp = QDateTime::currentDateTime().addSecs(-i);
        
        m_learningHistory.append(trainingExample);
    }
    
    qDebug() << "[BIO-NET] Donn√©es d'entrainement g√©n√©r√©es:" << m_learningHistory.size() << "exemples";
}
*/

// VERSION BITCOIN MINING
void BiologicalNetwork::generateTrainingData()
{
    qDebug() << "[BIO-NET] üéì G√©n√©ration de donn√©es d'entra√Ænement Bitcoin sp√©cialis√©es...";
    
    m_learningHistory.clear();
    
    // Configuration d'entra√Ænement progressif
    QVector<TrainingConfig> trainingLevels = {
        {1, 400, "D√©butant - 1 z√©ro"},
        {2, 300, "Interm√©diaire - 2 z√©ros"},
        {3, 200, "Avanc√© - 3 z√©ros"},  
        {4, 100, "Expert - 4 z√©ros"}
    };
    
    int totalExamples = 0;
    
    for (const TrainingConfig &level : trainingLevels) {
        qDebug() << "[BIO-NET] üìä G√©n√©ration niveau:" << level.description 
                 << "(" << level.exampleCount << "exemples)";
        
        for (int i = 0; i < level.exampleCount; ++i) {
            LearningData trainingExample = generateBitcoinTrainingExample(
                level.difficultyLevel, totalExamples + i
            );
            
            m_learningHistory.append(trainingExample);
            totalExamples++;
        }
    }
    
    // G√©n√©ration d'exemples de patterns sp√©ciaux
    generateSpecialBitcoinPatterns(100);
    
    // M√©lange pour √©viter l'overfitting s√©quentiel  
    shuffleTrainingData();
    
    qDebug() << "[BIO-NET] ‚úÖ Donn√©es d'entra√Ænement Bitcoin g√©n√©r√©es:"
             << m_learningHistory.size() << "exemples r√©alistes";
    
    // Statistiques d√©taill√©es
    logTrainingDataStatistics();
}


void BiologicalNetwork::stopLearning()
{
    QMutexLocker locker(&m_networkMutex);
    
    //m_learningTimer->stop();
    m_learningActive = false;
    // Emp√™che le thread de se joindre lui-m√™me (deadlock)
    if (m_learningThread.joinable() && std::this_thread::get_id() != m_learningThread.get_id()) {
        m_learningThread.join();
    }
    
    if (m_learningState == LearningState::InitialLearning || m_learningState == LearningState::Retraining) {
        if (m_currentEpoch >= m_totalEpochs * 0.8) { // Au moins 80% termin√©
            m_learningState = LearningState::Trained;
            qDebug() << "[BIO-NET] Apprentissage termin√© avec succ√®s";
            emit learningCompleted(true);
        } else {
            m_learningState = LearningState::Untrained;
            qDebug() << "[BIO-NET] Apprentissage interrompu";
            emit learningCompleted(false);
        }
    }
    
    emit learningStateChanged(m_learningState);
}

bool BiologicalNetwork::isLearningComplete() const
{
    QMutexLocker locker(&m_networkMutex);
    return m_learningState == LearningState::Trained || m_learningState == LearningState::Optimizing;
}

void BiologicalNetwork::onLearningCycle()
{
    QMutexLocker locker(&m_networkMutex);
    
    if (m_currentEpoch >= m_totalEpochs) {
        // Apprentissage termin√©
        //stopLearning();

        m_learningState = LearningState::Trained;
        qDebug() << "[BIO-NET] Apprentissage termin√© avec succ√®s";
        emit learningStateChanged(m_learningState);
        emit learningCompleted(true);
        m_learningActive = false;
        
        return;
    }
    
    // S√©lection d'un exemple d'apprentissage al√©atoire
    if (!m_learningHistory.isEmpty()) {
        int exampleIdx = QRandomGenerator::global()->bounded(m_learningHistory.size());
        const LearningData &example = m_learningHistory[exampleIdx];
        
        // Pr√©paration des targets bas√©s sur le nonce
        QVector<double> targets(32);
        uint64_t targetNonce = example.targetNonce;
        
        // Conversion du nonce en pattern binaire pour les neurones de sortie
        for (int bit = 0; bit < 32; ++bit) {
            targets[bit] = ((targetNonce >> bit) & 1) ? 1.0 : 0.0;
        }

        // Cycle d'apprentissage biologique
        performLearningCycle(example.inputSignals, targets);

        QVector<double> networkOutput = getNetworkOutput();
        uint64_t predictedNonce = 0;
        for (int bit = 0; bit < qMin(32, networkOutput.size()); ++bit) {
            if (networkOutput[bit] > 0.5) {
                predictedNonce |= (1ULL << bit);
            }
        }   

        if (m_currentEpoch % 10 == 0) {
                qDebug() << "apprentissage : sortie=" << networkOutput
                        << " targets=" << targets  
                        << " targetNonce=" << example.targetNonce << " -> predictNonce=" << predictedNonce; 
        }
        
        if (predictedNonce == example.targetNonce) {
            m_successfulPredictions++;
        }

        // predictions suite apprentissage
        m_totalPredictions++;
    }
    
    m_currentEpoch++;
    m_trainingProgress = static_cast<double>(m_currentEpoch) / m_totalEpochs;
    
    // √âmission p√©riodique du progr√®s
    if (m_currentEpoch % 10 == 0) {
        emit trainingProgress(m_trainingProgress * 100.0);
        
        // Calcul de l'efficacit√© du r√©seau
        calculateNetworkEfficiency();
        
        qDebug() << "[BIO-NET] Cycle" << m_currentEpoch << "/" << m_totalEpochs 
                 << "- Progression :" << QString::number(m_trainingProgress, 'f', 3)
                 << "- Success :" << QString::number(m_successfulPredictions, 'f', 3)
                 << "- Efficacit√©:" << QString::number(m_networkEfficiency, 'f', 3);
    }
}

/* ANCIENNE VERSION
void BiologicalNetwork::performLearningCycle(const QVector<double> &inputs, const QVector<double> &targets)
{
    // Forward propagation
    forwardPropagation(inputs);
    
    // Back propagation avec adaptation biologique
    backPropagation(targets);
    
    // Plasticit√© synaptique
    adjustSynapticWeights(m_config.learningRate);
    
    // Croissance neuronale adaptative
    if (m_currentEpoch % 50 == 0) {
        stimulateNeuronGrowth();
        pruneWeakConnections();
    }
}
*/

/* NOUVELLE METHODE */
void BiologicalNetwork::performLearningCycle(const QVector<double> &inputs, const QVector<double> &targets)
{
    // === ANALYSE PRE-APPRENTISSAGE BITCOIN ===
    BitcoinLearningContext context = analyzeBitcoinLearningContext(inputs, targets);
    
    // === FORWARD PROPAGATION AVEC MONITORING BITCOIN ===
    forwardPropagation(inputs);
    QVector<double> initialOutput = getNetworkOutput();
    
    // === BACK PROPAGATION ADAPTATIF BITCOIN ===
    performBitcoinBackPropagation(targets, context);
    
    // === AJUSTEMENT SYNAPTIQUE SP√âCIALIS√â ===
    adjustBitcoinSynapticWeights(context);
    
    // === VALIDATION ET CORRECTION BITCOIN ===
    BitcoinPredictionResult result = validateBitcoinPrediction(targets);
    
    // === RENFORCEMENT ADAPTATIF ===
    if (result.needsReinforcement) {
        applyBitcoinReinforcement(result, context);
    }
    
    // === CROISSANCE NEURONALE GUID√âE BITCOIN ===
    if (m_currentEpoch % 25 == 0) { // Plus fr√©quent pour Bitcoin
        stimulateBitcoinNeuronGrowth(context);
        pruneBitcoinWeakConnections(result);
    }
    
    // === M√âMORISATION DES PATTERNS R√âUSSIS ===
    if (result.isSuccessful) {
        memorizeBitcoinPattern(inputs, targets, result);
    }
    
    // === LOGGING D√âTAILL√â (OPTIONNEL) ===
    if (m_currentEpoch % 100 == 0) {
        logBitcoinLearningDetails(context, result);
    }
}

/**
 * @brief Analyse le contexte d'apprentissage Bitcoin
 */
BiologicalNetwork::BitcoinLearningContext BiologicalNetwork::analyzeBitcoinLearningContext(
    const QVector<double> &inputs, const QVector<double> &targets)
{
    BitcoinLearningContext context;
    
    // === EXTRACTION DU NONCE CIBLE ===
    context.targetNonce = 0;
    for (int bit = 0; bit < qMin(32, static_cast<int>(targets.size())); ++bit) {
        if (targets[bit] > 0.5) {
            context.targetNonce |= (1ULL << bit);
        }
    }
    
    // === ANALYSE DE LA DIFFICULT√â ===
    context.difficultyLevel = estimateDifficultyFromNonce(context.targetNonce);
    
    // === CALCUL DE LA COMPLEXIT√â DU PATTERN ===
    double variance = 0.0;
    double mean = std::accumulate(inputs.begin(), inputs.end(), 0.0) / inputs.size();
    for (double input : inputs) {
        variance += (input - mean) * (input - mean);
    }
    context.patternComplexity = sqrt(variance / inputs.size());
    
    // === D√âTECTION DE PATTERNS SP√âCIAUX ===
    context.isSpecialPattern = false; // A REVOIR avec header.version : detectSpecialBitcoinPattern(inputs);
    
    // === TAUX D'APPRENTISSAGE ADAPTATIF ===
    context.adaptiveLearningRate = calculateAdaptiveLearningRate(context);
    
    // === IDENTIFICATION DES BITS CRITIQUES ===
    context.criticalBits = identifyCriticalBits(context.targetNonce);
    
    // === CONFIANCE ATTENDUE ===
    context.expectedConfidence = estimateExpectedConfidence(context);
    
    return context;
}

/**
 * @brief Back propagation sp√©cialis√©e Bitcoin
 */
void BiologicalNetwork::performBitcoinBackPropagation(const QVector<double> &targets, const BitcoinLearningContext &context)
{
    // Back propagation standard
    backPropagation(targets);
    
    // === CORRECTIONS SP√âCIFIQUES BITCOIN ===
    
    // 1. Emphasis sur les bits critiques
    NetworkLayer &outputLayer = m_layers.last();
    for (int criticalBit : context.criticalBits) {
        if (criticalBit < outputLayer.neurons.size()) {
            BiologicalNeuron &neuron = outputLayer.neurons[criticalBit];
            
            // Augmenter l'influence des bits critiques
            double targetValue = targets[criticalBit];
            double currentOutput = neuron.activation;
            double criticalError = (targetValue - currentOutput) * 1.5; // Emphasis x1.5
            
            // Ajustement direct du seuil pour les bits critiques
            neuron.threshold += criticalError * 0.02;
            neuron.threshold = qBound(0.1, neuron.threshold, 0.9);
        }
    }
    
    // 2. Ajustement bas√© sur la difficult√©
    double difficultyMultiplier = 1.0 + (context.difficultyLevel * 0.3);
    for (NetworkLayer &layer : m_layers) {
        for (BiologicalNeuron &neuron : layer.neurons) {
            neuron.threshold *= difficultyMultiplier;
            neuron.threshold = qBound(0.05, neuron.threshold, 0.95);
        }
    }
}

/**
 * @brief Ajustement synaptique sp√©cialis√© Bitcoin
 */
void BiologicalNetwork::adjustBitcoinSynapticWeights(const BitcoinLearningContext &context)
{
    // Utiliser le taux d'apprentissage adaptatif
    adjustSynapticWeights(context.adaptiveLearningRate);
    
    // === RENFORCEMENTS SP√âCIALIS√âS BITCOIN ===
    
    // 1. Renforcement des connexions vers les bits critiques
    if (m_layers.size() >= 2) {
        NetworkLayer &outputLayer = m_layers.last();
        NetworkLayer &prevLayer = m_layers[m_layers.size() - 2];
        
        for (int criticalBit : context.criticalBits) {
            if (criticalBit < outputLayer.neurons.size()) {
                BiologicalNeuron &criticalNeuron = outputLayer.neurons[criticalBit];
                
                // Renforcer les connexions entrantes vers les neurones critiques
                for (int weightIdx = 0; weightIdx < criticalNeuron.weights.size(); ++weightIdx) {
                    double preActivation = prevLayer.neurons[weightIdx].activation;
                    if (preActivation > 0.5) { // Neurone pr√©-synaptique actif
                        criticalNeuron.weights[weightIdx] += context.adaptiveLearningRate * 0.1;
                    }
                }
            }
        }
    }
    
    // 2. Modulation par complexit√© du pattern
    if (context.patternComplexity > 0.5) {
        // Pattern complexe : apprentissage plus conservateur
        for (NetworkLayer &layer : m_layers) {
            for (BiologicalNeuron &neuron : layer.neurons) {
                for (double &weight : neuron.weights) {
                    weight *= 0.99; // L√©g√®re r√©duction pour stabilit√©
                }
            }
        }
    }
}

BiologicalNetwork::BitcoinPredictionResult BiologicalNetwork::validateBitcoinPrediction(const QVector<double> &targets)
{
    BitcoinPredictionResult result;
    
    // Reconstruction du nonce cible
    uint64_t targetNonce = 0;
    for (int bit = 0; bit < qMin(32, static_cast<int>(targets.size())); ++bit) {
        if (targets[bit] > 0.5) {
            targetNonce |= (1ULL << bit);
        }
    }
    
    // Reconstruction du nonce pr√©dit
    QVector<double> networkOutput = getNetworkOutput();
    result.predictedNonce = 0;
    for (int bit = 0; bit < qMin(32, static_cast<int>(networkOutput.size())); ++bit) {
        if (networkOutput[bit] > 0.5) {
            result.predictedNonce |= (1ULL << bit);
        }
    }
    
    // Calculs de pr√©cision
    uint64_t xorResult = result.predictedNonce ^ targetNonce;
    result.correctBits = 32 - __builtin_popcountll(xorResult);
    result.accuracy = static_cast<double>(result.correctBits) / 32.0;
    
    // Succ√®s si match exact ou tr√®s proche
    result.isSuccessful = (result.accuracy >= 0.95);
    
    // Renforcement n√©cessaire si performance faible
    result.needsReinforcement = (result.accuracy < 0.7);
    
    // Confiance du r√©seau
    double totalConfidence = 0.0;
    for (double output : networkOutput) {
        totalConfidence += qMax(output, 1.0 - output); // Distance √† 0.5
    }
    result.confidence = (totalConfidence / networkOutput.size() - 0.5) * 2.0;
    
    // Analyse d'erreur
    if (result.accuracy < 0.5) {
        result.errorAnalysis = "POOR_PATTERN_RECOGNITION";
    } else if (result.accuracy < 0.8) {
        result.errorAnalysis = "PARTIAL_LEARNING";
    } else {
        result.errorAnalysis = "GOOD_CONVERGENCE";
    }
    
    return result;
}

/**
 * @brief Applique le renforcement Bitcoin
 */
void BiologicalNetwork::applyBitcoinReinforcement(const BitcoinPredictionResult &result, const BitcoinLearningContext &context)
{
    if (result.isSuccessful) {
        // === RENFORCEMENT POSITIF ===
        
        // 1. Augmenter les seuils des neurones performants
        for (NetworkLayer &layer : m_layers) {
            for (BiologicalNeuron &neuron : layer.neurons) {
                if (neuron.activation > 0.7) {
                    neuron.adaptationFactor = qMin(2.0, neuron.adaptationFactor * 1.05);
                }
            }
        }
        
        // 2. Renforcer les connexions synaptiques r√©ussies
        for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
            NetworkLayer &layer = m_layers[layerIdx];
            for (int neuronIdx = 0; neuronIdx < layer.neurons.size(); ++neuronIdx) {
                for (int synapseIdx = 0; synapseIdx < layer.synapses[neuronIdx].size(); ++synapseIdx) {
                    if (layer.synapses[neuronIdx][synapseIdx] > 0.6) {
                        layer.synapses[neuronIdx][synapseIdx] *= 1.02; // Renforcement 2%
                    }
                }
            }
        }
        
    } else if (result.needsReinforcement) {
        // === CORRECTION N√âGATIVE ===
        
        // 1. R√©ajuster les neurones sur-confiants
        QVector<double> networkOutput = getNetworkOutput();
        for (int bit = 0; bit < qMin(32, static_cast<int>(networkOutput.size())); ++bit) {
            if (m_layers.size() > 0) {
                NetworkLayer &outputLayer = m_layers.last();
                if (bit < outputLayer.neurons.size()) {
                    BiologicalNeuron &neuron = outputLayer.neurons[bit];
                    
                    // Si le neurone √©tait tr√®s confiant mais faux
                    if ((networkOutput[bit] > 0.8 || networkOutput[bit] < 0.2) && result.correctBits < 20) {
                        neuron.threshold += (0.5 - networkOutput[bit]) * 0.05; // Ramener vers 0.5
                        neuron.threshold = qBound(0.1, neuron.threshold, 0.9);
                    }
                }
            }
        }
        
        // 2. Affaiblir les connexions dominantes d√©faillantes
        for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
            NetworkLayer &layer = m_layers[layerIdx];
            for (int neuronIdx = 0; neuronIdx < layer.neurons.size(); ++neuronIdx) {
                for (int synapseIdx = 0; synapseIdx < layer.synapses[neuronIdx].size(); ++synapseIdx) {
                    if (layer.synapses[neuronIdx][synapseIdx] > 0.8) {
                        layer.synapses[neuronIdx][synapseIdx] *= 0.95; // Affaiblissement 5%
                    }
                }
            }
        }
    }
}

/**
 * @brief Croissance neuronale guid√©e par Bitcoin
 */
void BiologicalNetwork::stimulateBitcoinNeuronGrowth(const BitcoinLearningContext &context)
{
    // Croissance plus agressive pour les patterns difficiles
    double growthFactor = 1.0 + (context.difficultyLevel * 0.1);
    
    for (NetworkLayer &layer : m_layers) {
        for (BiologicalNeuron &neuron : layer.neurons) {
            
            // Crit√®res de croissance Bitcoin-sp√©cifiques
            bool shouldGrow = false;
            
            if (neuron.activation > 0.8 && neuron.bitcoin_response_score > 0.6) {
                shouldGrow = true; // Neurone performant pour Bitcoin
            }
            
            if (context.isSpecialPattern && neuron.activation > 0.6) {
                shouldGrow = true; // Pattern sp√©cial n√©cessite plus de capacit√©
            }
            
            if (shouldGrow) {
                neuron.adaptationFactor = qMin(2.0, static_cast<double>(neuron.adaptationFactor * (1.0 + growthFactor * 0.01)));
                
                // Augmenter la connectivit√© pour les neurones Bitcoin performants
                if (neuron.connectionCount < neuron.weights.size() * 1.4) {
                    neuron.adaptationFactor *= 1.005;
                }
            }
        }
    }
}

/**
 * @brief √âlagage sp√©cialis√© Bitcoin
 */
void BiologicalNetwork::pruneBitcoinWeakConnections(const BitcoinPredictionResult &result)
{
    // √âlagage plus agressif si les performances Bitcoin sont faibles
    double pruningThreshold = MIN_CONNECTION_STRENGTH;
    
    if (result.accuracy < 0.5) {
        pruningThreshold *= 1.5; // √âlagage plus agressif
    }
    
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &layer = m_layers[layerIdx];
        
        for (int neuronIdx = 0; neuronIdx < layer.neurons.size(); ++neuronIdx) {
            for (int synapseIdx = 0; synapseIdx < layer.synapses[neuronIdx].size(); ++synapseIdx) {
                
                if (layer.synapses[neuronIdx][synapseIdx] < pruningThreshold) {
                    
                    // √âlagage diff√©rentiel selon performance Bitcoin
                    if (result.accuracy < 0.3) {
                        layer.synapses[neuronIdx][synapseIdx] *= 0.7; // √âlagage fort
                    } else {
                        layer.synapses[neuronIdx][synapseIdx] *= 0.9; // √âlagage normal
                    }
                }
            }
        }
    }
}

/**
 * @brief M√©morise les patterns Bitcoin r√©ussis
 */
void BiologicalNetwork::memorizeBitcoinPattern(const QVector<double> &inputs, const QVector<double> &targets, const BitcoinPredictionResult &result)
{
    // Cr√©er un snapshot du r√©seau pour ce pattern r√©ussi
    BitcoinPatternMemory memory;
    memory.inputPattern = inputs;
    memory.targetPattern = targets;
    memory.networkSnapshot = captureNetworkSnapshot();
    memory.successTimestamp = QDateTime::currentDateTime();
    memory.accuracy = result.accuracy;
    memory.confidence = result.confidence;
    
    // Stocker dans la m√©moire des patterns r√©ussis
    m_bitcoinPatternMemory.append(memory);
    
    // Limiter la taille de la m√©moire
    while (m_bitcoinPatternMemory.size() > 100) {
        m_bitcoinPatternMemory.removeFirst();
    }
}

/**
 * @brief Calcule un taux d'apprentissage adaptatif
 */
double BiologicalNetwork::calculateAdaptiveLearningRate(const BitcoinLearningContext &context)
{
    double baseLearningRate = m_config.learningRate;
    
    // Modulation par difficult√©
    double difficultyFactor = 1.0 + (context.difficultyLevel * 0.2);
    
    // Modulation par complexit√©
    double complexityFactor = 1.0 + (context.patternComplexity * 0.3);
    
    // Modulation pour patterns sp√©ciaux
    double specialFactor = context.isSpecialPattern ? 1.5 : 1.0;
    
    // Modulation par performance r√©cente
    double performanceFactor = 1.0;
    if (m_totalPredictions > 10) {
        double recentSuccessRate = static_cast<double>(m_successfulPredictions) / m_totalPredictions;
        if (recentSuccessRate < 0.3) {
            performanceFactor = 1.3; // Augmenter si performance faible
        } else if (recentSuccessRate > 0.8) {
            performanceFactor = 0.8; // Diminuer si performance tr√®s bonne
        }
    }
    
    double adaptiveRate = baseLearningRate * difficultyFactor * complexityFactor * specialFactor * performanceFactor;
    
    // Borner le taux d'apprentissage
    return qBound(0.0001, adaptiveRate, 0.1);
}

/**
 * @brief Identifie les bits critiques d'un nonce
 */
QVector<int> BiologicalNetwork::identifyCriticalBits(uint64_t nonce)
{
    QVector<int> criticalBits;
    
    // Les bits de poids fort sont g√©n√©ralement plus critiques
    for (int bit = 24; bit < 32; ++bit) {
        if ((nonce >> bit) & 1) {
            criticalBits.append(bit);
        }
    }
    
    // Ajouter des bits avec des patterns particuliers
    for (int bit = 0; bit < 32; bit += 4) {
        uint8_t nibble = (nonce >> bit) & 0xF;
        if (nibble == 0x0 || nibble == 0xF) { // Nibbles tout 0 ou tout 1
            for (int subBit = 0; subBit < 4; ++subBit) {
                criticalBits.append(bit + subBit);
            }
        }
    }
    
    return criticalBits;
}

/**
 * @brief Logging d√©taill√© de l'apprentissage Bitcoin
 */
void BiologicalNetwork::logBitcoinLearningDetails(const BitcoinLearningContext &context, const BitcoinPredictionResult &result)
{
    qDebug() << "[BIO-NET] üß¨‚Çø === BITCOIN LEARNING CYCLE DETAILS ===";
    qDebug() << "  üéØ Target Nonce:" << QString("0x%1").arg(context.targetNonce, 8, 16, QChar('0'));
    qDebug() << "  üìä Difficulty Level:" << context.difficultyLevel;
    qDebug() << "  üåä Pattern Complexity:" << QString::number(context.patternComplexity, 'f', 3);
    qDebug() << "  ‚≠ê Special Pattern:" << (context.isSpecialPattern ? "YES" : "NO");
    qDebug() << "  üìà Adaptive LR:" << QString::number(context.adaptiveLearningRate, 'f', 6);
    qDebug() << "  üéØ Predicted Nonce:" << QString("0x%1").arg(result.predictedNonce, 8, 16, QChar('0'));
    qDebug() << "  ‚úÖ Correct Bits:" << result.correctBits << "/32";
    qDebug() << "  üìä Accuracy:" << QString::number(result.accuracy * 100, 'f', 1) << "%";
    qDebug() << "  üé™ Confidence:" << QString::number(result.confidence * 100, 'f', 1) << "%";
    qDebug() << "  üîç Error Analysis:" << result.errorAnalysis;
    qDebug() << "  üèÖ Critical Bits:" << context.criticalBits.size();
}



void BiologicalNetwork::forwardPropagation(const QVector<double> &inputs)
{
    // Couche d'entr√©e
    if (inputs.size() == m_layers[0].neurons.size()) {
        for (int i = 0; i < inputs.size(); ++i) {
            m_layers[0].neurons[i].activation = inputs[i];
            m_layers[0].neurons[i].lastStimulation = QDateTime::currentMSecsSinceEpoch();
        }
    }
    
    // Propagation √† travers les couches cach√©es et de sortie
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &currentLayer = m_layers[layerIdx];
        const NetworkLayer &prevLayer = m_layers[layerIdx - 1];
        
        currentLayer.layerActivation = 0.0;

        //if (m_currentEpoch % 10 == 0) {
        //    qDebug() << "[BIO-NET][DEBUG] --- Layer" << layerIdx << "(" << currentLayer.layerType << ") ---";
        //}
        
        for (int neuronIdx = 0; neuronIdx < currentLayer.neurons.size(); ++neuronIdx) {
            BiologicalNeuron &neuron = currentLayer.neurons[neuronIdx];
            
            // Calcul de l'entr√©e pond√©r√©e
            double weightedSum = 0.0;
            for (int prevIdx = 0; prevIdx < prevLayer.neurons.size(); ++prevIdx) {
                double synapticInput = prevLayer.neurons[prevIdx].activation * neuron.weights[prevIdx];
                
                // Modulation synaptique biologique
                double synapticStrength = currentLayer.synapses[neuronIdx][prevIdx];
                synapticInput *= synapticStrength;
                
                neuron.dendrites[prevIdx] = synapticInput;
                weightedSum += synapticInput;
            }
            
            // Fonction d'activation biologique avec seuil adaptatif
            double threshold = adaptiveThreshold(neuron);
            neuron.activation = biologicalActivation(weightedSum, threshold);
            
            // Fatigue neuronale
            neuron.activation *= NEURON_FATIGUE_FACTOR;
            
            currentLayer.layerActivation += neuron.activation;

           // --- LOG PAR NEURONE ---
           // if (m_currentEpoch % 10 == 0 && layerIdx == 1) {
           //     qDebug() << "  neuron" << neuronIdx
           //              //<< "weights:" << neuron.weights
           //              << "weightedSum:" << weightedSum
           //              << "threshold:" << threshold
           //              << "activationBeforeFatigue:" << biologicalActivation(weightedSum, threshold)
           //              << "activation:" << neuron.activation;
           //}        
        }
        
        currentLayer.layerActivation /= currentLayer.neurons.size();

        //if (m_currentEpoch % 10 == 0) {        
        //    qDebug() << "[BIO-NET][DEBUG] Layer" << layerIdx << "mean activation:" << currentLayer.layerActivation;
        //}
     
    }

    // --- LOG OUTPUT ---
    //if (m_currentEpoch % 10 == 0) {
    //    qDebug() << "[BIO-NET][DEBUG] Output layer activations:";
    //    for (int i = 0; i < m_layers.last().neurons.size(); ++i) {
    //        qDebug() << "  neuron" << i << "activation:" << m_layers.last().neurons[i].activation;
    //    }
    //}
    
    QStringList inputsTmp;
    QStringList outputsTmp;
    for (const auto& neuron : m_layers[0].neurons) {
        inputsTmp << QString::number(neuron.activation, 'f', 3);
    }
    for (const auto& neuron : m_layers.last().neurons) {
        outputsTmp << QString::number(neuron.activation, 'f', 3);
    }
    
    // √âmission p√©riodique du progr√®s
    if (m_currentEpoch % 10 == 0) {
        qDebug() << "[BIO-NET] Cycle forwardPropagation"
                 << "- Input :" << inputsTmp
                 << "- Sortie:" << outputsTmp;
    }

}

void BiologicalNetwork::backPropagation(const QVector<double> &targets)
{
    // Calcul de l'erreur sur la couche de sortie
    NetworkLayer &outputLayer = m_layers.last();
    QVector<double> outputErrors(outputLayer.neurons.size());
    
    for (int i = 0; i < outputLayer.neurons.size() && i < targets.size(); ++i) {
        double error = targets[i] - outputLayer.neurons[i].activation;
        outputErrors[i] = error * sigmoidDerivative(outputLayer.neurons[i].activation);
        
        // Adaptation biologique du seuil
        BiologicalNeuron &neuron = outputLayer.neurons[i];
        neuron.threshold += error * 0.01; // Micro-adaptation
        neuron.threshold = qBound(0.1, static_cast<double>(neuron.threshold), 0.9);
    }

    // √âmission p√©riodique du progr√®s
    if (m_currentEpoch % 10 == 0) {
        qDebug() << "[BIO-NET] Cycle backPropagation"
                 << "- outputErrors :" << outputErrors;
    }

    
    // R√©tropropagation de l'erreur vers les couches cach√©es
    for (int layerIdx = m_layers.size() - 2; layerIdx >= 1; --layerIdx) {
        NetworkLayer &currentLayer = m_layers[layerIdx];
        const NetworkLayer &nextLayer = m_layers[layerIdx + 1];
        
        QVector<double> layerErrors(currentLayer.neurons.size(), 0.0);
        
        for (int neuronIdx = 0; neuronIdx < currentLayer.neurons.size(); ++neuronIdx) {
            double errorSum = 0.0;
            
            for (int nextIdx = 0; nextIdx < nextLayer.neurons.size(); ++nextIdx) {
                errorSum += outputErrors[nextIdx] * nextLayer.neurons[nextIdx].weights[neuronIdx];
            }
            
            layerErrors[neuronIdx] = errorSum * sigmoidDerivative(currentLayer.neurons[neuronIdx].activation);
            
            // Adaptation du seuil biologique
            currentLayer.neurons[neuronIdx].threshold += layerErrors[neuronIdx] * 0.005;
            currentLayer.neurons[neuronIdx].threshold = qBound(0.1, static_cast<double>(currentLayer.neurons[neuronIdx].threshold), 0.9);
        }
        
        outputErrors = layerErrors; // Pour la prochaine it√©ration
    }

}

void BiologicalNetwork::adjustSynapticWeights(double learningSignal)
{
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &currentLayer = m_layers[layerIdx];
        const NetworkLayer &prevLayer = m_layers[layerIdx - 1];
        
        for (int neuronIdx = 0; neuronIdx < currentLayer.neurons.size(); ++neuronIdx) {
            BiologicalNeuron &neuron = currentLayer.neurons[neuronIdx];
            
            for (int weightIdx = 0; weightIdx < neuron.weights.size(); ++weightIdx) {
                // R√®gle d'apprentissage hebbienne modifi√©e
                double preActivation = prevLayer.neurons[weightIdx].activation;
                double postActivation = neuron.activation;
                
                // Delta de poids avec plasticit√© synaptique
                double weightDelta = learningSignal * preActivation * postActivation * SYNAPTIC_PLASTICITY_RATE;
                
                // Modification biologique des connexions synaptiques
                neuron.weights[weightIdx] += weightDelta;
                currentLayer.synapses[neuronIdx][weightIdx] += weightDelta * 0.1;
                
                // Contraintes biologiques
                neuron.weights[weightIdx] = qBound(-2.0, static_cast<double>(neuron.weights[weightIdx]), 2.0);
                currentLayer.synapses[neuronIdx][weightIdx] = qBound(MIN_CONNECTION_STRENGTH, 
                                                                   static_cast<double>(currentLayer.synapses[neuronIdx][weightIdx]), 1.0);
            }
        }
    }
}

double BiologicalNetwork::sigmoidActivation(double input) const
{
    return 1.0 / (1.0 + exp(-input));
}

double BiologicalNetwork::sigmoidDerivative(double sigmoidOutput) const
{
    return sigmoidOutput * (1.0 - sigmoidOutput);
}

double BiologicalNetwork::biologicalActivation(double input, double threshold) const
{
    // Fonction d'activation biologique avec seuil et saturation
    if (input < threshold) {
        double alpha = 0.01; // pente faible pour input < threshold
        if (input < threshold)
            return alpha * (input - threshold);        
        //return 0.0;
    }
    
    // Activation sigmo√Ødale avec d√©calage par le seuil
    double adjustedInput = (input - threshold) * 2.0;
    return sigmoidActivation(adjustedInput);
}

double BiologicalNetwork::adaptiveThreshold(const BiologicalNeuron &neuron) const
{
    // Seuil adaptatif bas√© sur l'historique et la fatigue
    double baseThreshold = neuron.threshold;
    double adaptationFactor = neuron.adaptationFactor;
    
    // Adaptation temporelle (fatigue/r√©cup√©ration)
    qint64 timeSinceStimulation = QDateTime::currentMSecsSinceEpoch() - neuron.lastStimulation;
    double recoveryFactor = qMin(1.0, timeSinceStimulation / 1000.0); // R√©cup√©ration en 1 seconde
    
    return baseThreshold * adaptationFactor * (1.0 - recoveryFactor * 0.2);
}

void BiologicalNetwork::stimulateNeuronGrowth()
{
    // Croissance des connexions pour les neurones tr√®s actifs
    for (NetworkLayer &layer : m_layers) {
        for (BiologicalNeuron &neuron : layer.neurons) {
            if (neuron.activation > 0.8 && neuron.connectionCount < neuron.weights.size() * 1.2) {
                // Croissance dendritique
                neuron.adaptationFactor = qMin(1.2, neuron.adaptationFactor * 1.01);
            }
        }
    }
}

void BiologicalNetwork::pruneWeakConnections()
{
    // √âlagage des connexions faibles pour optimiser le r√©seau
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &layer = m_layers[layerIdx];
        
        for (int neuronIdx = 0; neuronIdx < layer.neurons.size(); ++neuronIdx) {
            for (int synapseIdx = 0; synapseIdx < layer.synapses[neuronIdx].size(); ++synapseIdx) {
                if (layer.synapses[neuronIdx][synapseIdx] < MIN_CONNECTION_STRENGTH * 0.5) {
                    // Affaiblissement des connexions tr√®s faibles
                    layer.synapses[neuronIdx][synapseIdx] *= 0.9;
                }
            }
        }
    }
}

void BiologicalNetwork::calculateNetworkEfficiency()
{
    if (m_totalPredictions == 0) {
        m_networkEfficiency = 0.0;
        return;
    }
    
    double successRate = static_cast<double>(m_successfulPredictions) / m_totalPredictions;
    double confidenceRate = m_averageConfidence;
    double trainingRate = m_trainingProgress;
    
    m_networkEfficiency = (successRate * 0.5 + confidenceRate * 0.3 + trainingRate * 0.2);
    
    // Bonus pour stabilit√© des connexions
    double stabilityBonus = calculateNetworkStability();
    m_networkEfficiency = qMin(1.0, m_networkEfficiency + stabilityBonus * 0.1);
}

double BiologicalNetwork::calculateNetworkStability() const
{
    double totalStability = 0.0;
    int connectionCount = 0;
    
    for (const NetworkLayer &layer : m_layers) {
        for (const auto &synapseRow : layer.synapses) {
            for (double synapseStrength : synapseRow) {
                // Stabilit√© = proximit√© des forces synaptiques √† leurs valeurs optimales
                double optimalRange = 0.6; // Force synaptique optimale
                double stability = 1.0 - abs(synapseStrength - optimalRange);
                totalStability += qMax(0.0, stability);
                connectionCount++;
            }
        }
    }
    
    return connectionCount > 0 ? totalStability / connectionCount : 0.0;
}

BiologicalNetwork::NoncePredicition BiologicalNetwork::predictOptimalNonce(
    const QString &blockHeader, uint64_t difficulty, const QVector<double> &currentSignals)
{
    QMutexLocker locker(&m_networkMutex);
    
    NoncePredicition prediction;
    prediction.confidence = 0.0;
    prediction.expectedEfficiency = 0.0;
    prediction.reasoning = "R√©seau non entrain√©";
    
    if (m_learningState != LearningState::Trained && m_learningState != LearningState::Optimizing) {
        prediction.suggestedNonce = QRandomGenerator::global()->generate64() % (1ULL << 32);
        return prediction;
    }
    
    // Forward propagation avec les signaux actuels
    forwardPropagation(currentSignals);
    
    // R√©cup√©ration de la sortie du r√©seau
    QVector<double> networkOutput = getNetworkOutput();
    
    // Conversion de la sortie en nonce
    uint64_t predictedNonce = 0;
    for (int bit = 0; bit < qMin(32, networkOutput.size()); ++bit) {
        if (networkOutput[bit] > 0.5) {
            predictedNonce |= (1ULL << bit);
        }
    }
    
    prediction.suggestedNonce = predictedNonce;
    
    // Calcul de la confiance bas√© sur la coh√©rence de la sortie
    double outputVariance = 0.0;
    double outputMean = std::accumulate(networkOutput.begin(), networkOutput.end(), 0.0) / networkOutput.size();
    
    for (double output : networkOutput) {
        outputVariance += (output - outputMean) * (output - outputMean);
    }
    outputVariance /= networkOutput.size();
    
    // Confiance inversement proportionnelle √† la variance (plus c'est d√©cisif, plus on est confiant)
    prediction.confidence = qMax(0.1, 1.0 - sqrt(outputVariance));
    
    // G√©n√©ration de nonces candidats dans la r√©gion pr√©dite
    prediction.candidateRange = generateNonceCandidates(networkOutput);
    
    // Estimation de l'efficacit√© bas√©e sur l'historique
    prediction.expectedEfficiency = estimateNonceEfficiency(prediction.suggestedNonce, difficulty);
    
    // Explication de la pr√©diction
    prediction.reasoning = QString("R√©seau bio-neuronal - Confiance: %1% - R√©gion optimale: 0x%2")
                          .arg(prediction.confidence * 100, 0, 'f', 1)
                          .arg(prediction.suggestedNonce, 8, 16, QChar('0'));
    
    // Mise √† jour des statistiques
    m_totalPredictions++;
    m_averageConfidence = ((m_averageConfidence * (m_totalPredictions - 1)) + prediction.confidence) / m_totalPredictions;
    
    qDebug() << "[BIO-NET] Pr√©diction nonce: 0x" << QString::number(prediction.suggestedNonce, 16)
             << "Confiance:" << QString::number(prediction.confidence * 100, 'f', 1) << "%";
    
    emit noncePredictionReady(prediction);
    
    return prediction;
}

QVector<uint64_t> BiologicalNetwork::generateNonceCandidates(const QVector<double> &networkOutput)
{
    QVector<uint64_t> candidates;
    
    uint64_t baseNonce = 0;
    for (int bit = 0; bit < qMin(32, networkOutput.size()); ++bit) {
        if (networkOutput[bit] > 0.5) {
            baseNonce |= (1ULL << bit);
        }
    }
    
    // G√©n√©ration de variations autour de la pr√©diction principale
    for (int variation = 0; variation < 16; ++variation) {
        uint64_t candidate = baseNonce;
        
        // Variations bas√©es sur les activations proches du seuil
        for (int bit = 0; bit < networkOutput.size(); ++bit) {
            double activation = networkOutput[bit];
            if (activation > 0.4 && activation < 0.6) { // Zone d'incertitude
                if (QRandomGenerator::global()->generateDouble() < 0.3) {
                    candidate ^= (1ULL << bit); // Flip du bit
                }
            }
        }
        
        candidates.append(candidate);
    }
    
    return candidates;
}

double BiologicalNetwork::estimateNonceEfficiency(uint64_t nonce, uint64_t difficulty)
{
    // Estimation bas√©e sur l'historique des performances
    double efficiency = 0.5; // Base neutre
    
    // Recherche de patterns similaires dans l'historique
    for (const LearningData &example : m_learningHistory) {
        if (example.difficulty == difficulty || abs((int64_t)example.difficulty - (int64_t)difficulty) < (int64_t)difficulty * 0.1) {
            // Similarit√© de nonce (bits communs)
            uint64_t commonBits = ~(nonce ^ example.targetNonce);
            int similarity = __builtin_popcountll(commonBits);
            
            if (similarity > 16) { // Plus de 50% de bits communs
                double weight = similarity / 64.0;
                if (example.wasSuccessful) {
                    efficiency += weight * 0.3;
                } else {
                    efficiency -= weight * 0.1;
                }
            }
        }
    }
    
    return qBound(0.0, efficiency, 1.0);
}

void BiologicalNetwork::addLearningExample(const LearningData &data)
{
    QMutexLocker locker(&m_networkMutex);
    
    m_learningHistory.append(data);
    
    // Stockage dans la m√©moire de patterns
    storePatternMemory(data);
    
    // Limite de l'historique pour √©viter la surconsommation m√©moire
    while (m_learningHistory.size() > MAX_PATTERN_MEMORY) {
        m_learningHistory.removeFirst();
    }
    
    qDebug() << "[BIO-NET] Exemple d'apprentissage ajout√© - Succ√®s:" << data.wasSuccessful
             << "Nonce: 0x" << QString::number(data.targetNonce, 16);
}

void BiologicalNetwork::storePatternMemory(const LearningData &data)
{
    // Stockage s√©lectif des patterns les plus int√©ressants
    bool shouldStore = false;
    
    if (data.wasSuccessful) {
        shouldStore = true; // Tous les succ√®s
    } else if (data.attempts < 1000) {
        shouldStore = true; // √âchecs rapides (potentiellement informatifs)
    } else if (QRandomGenerator::global()->generateDouble() < 0.1) {
        shouldStore = true; // 10% des autres √©checs
    }
    
    if (shouldStore) {
        m_patternMemory.append(data);
        
        while (m_patternMemory.size() > MAX_PATTERN_MEMORY / 10) {
            m_patternMemory.removeFirst();
        }
    }
}

void BiologicalNetwork::performRetroLearning()
{
    QMutexLocker locker(&m_networkMutex);
    
    if (m_learningHistory.size() < 10) {
        qDebug() << "[BIO-NET] Pas assez de donn√©es pour le r√©tro-apprentissage";
        return;
    }
    
    m_learningState = LearningState::Retraining;
    emit learningStateChanged(m_learningState);
    
    qDebug() << "[BIO-NET] D√©marrage r√©tro-apprentissage avec" << m_learningHistory.size() << "exemples";
    
    // S√©lection des exemples les plus r√©cents et pertinents
    QVector<LearningData> recentExamples;
    int maxExamples = qMin(100, m_learningHistory.size());
    
    for (int i = m_learningHistory.size() - maxExamples; i < m_learningHistory.size(); ++i) {
        recentExamples.append(m_learningHistory[i]);
    }
    
    // Cycles de r√©tro-apprentissage
    for (int cycle = 0; cycle < 20; ++cycle) {
        for (const LearningData &example : recentExamples) {
            QVector<double> targets(32);
            uint64_t targetNonce = example.targetNonce;
            
            for (int bit = 0; bit < 32; ++bit) {
                targets[bit] = ((targetNonce >> bit) & 1) ? 1.0 : 0.0;
            }
            
            // Pond√©ration de l'apprentissage selon le succ√®s
            double learningWeight = example.wasSuccessful ? 1.0 : 0.3;
            
            // Apprentissage avec pond√©ration
            forwardPropagation(example.inputSignals);
            backPropagation(targets);
            adjustSynapticWeights(m_config.learningRate * learningWeight);
        }
        
        // Adaptation des seuils
        if (cycle % 5 == 0) {
            adaptNetworkThresholds();
        }
    }
    
    m_learningState = LearningState::Trained;
    emit learningStateChanged(m_learningState);
    
    calculateNetworkEfficiency();
    
    qDebug() << "[BIO-NET] R√©tro-apprentissage termin√© - Nouvelle efficacit√©:" 
             << QString::number(m_networkEfficiency, 'f', 3);
    
    emit networkOptimized(m_networkEfficiency);
}

void BiologicalNetwork::adaptNetworkThresholds()
{
    // Adaptation des seuils bas√©e sur l'activit√© moyenne des neurones
    for (NetworkLayer &layer : m_layers) {
        double layerActivity = 0.0;
        
        for (const BiologicalNeuron &neuron : layer.neurons) {
            layerActivity += neuron.activation;
        }
        
        layerActivity /= layer.neurons.size();
        
        // Ajustement des seuils pour maintenir une activit√© optimale
        double targetActivity = 0.3; // 30% des neurones actifs
        double thresholdAdjustment = (layerActivity - targetActivity) * 0.01;
        
        for (BiologicalNeuron &neuron : layer.neurons) {
            neuron.threshold += thresholdAdjustment;
            neuron.threshold = qBound(0.1, static_cast<double>(neuron.threshold), 0.9);
        }
    }
}

void BiologicalNetwork::optimizeFromFeedback(bool success, const LearningData &context)
{
    QMutexLocker locker(&m_networkMutex);
    
    if (success) {
        m_successfulPredictions++;
        reinforceSuccessfulPaths(context);
        
        qDebug() << "[BIO-NET] Renforcement des chemins r√©ussis - Nonce: 0x"
                 << QString::number(context.targetNonce, 16);
    } else {
        weakenFailurePaths(context);
        
        qDebug() << "[BIO-NET] Affaiblissement des chemins √©chou√©s";
    }
    
    // Mise √† jour de l'efficacit√©
    calculateNetworkEfficiency();
    
    // D√©clenchement d'optimisation si n√©cessaire
    if (m_totalPredictions % 50 == 0) {
        if (m_networkEfficiency < 0.3) {
            qDebug() << "[BIO-NET] Efficacit√© faible, d√©clenchement r√©tro-apprentissage";
            performRetroLearning();
        }
    }
}

void BiologicalNetwork::reinforceSuccessfulPaths(const LearningData &successData)
{
    // Renforcement des connexions ayant contribu√© au succ√®s
    
    // Simulation du chemin de succ√®s
    forwardPropagation(successData.inputSignals);
    
    // Renforcement des synapses actives
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &layer = m_layers[layerIdx];
        
        for (int neuronIdx = 0; neuronIdx < layer.neurons.size(); ++neuronIdx) {
            BiologicalNeuron &neuron = layer.neurons[neuronIdx];
            
            if (neuron.activation > 0.6) { // Neurone fortement activ√©
                // Renforcement des poids entrants
                for (int weightIdx = 0; weightIdx < neuron.weights.size(); ++weightIdx) {
                    neuron.weights[weightIdx] *= 1.05; // Renforcement 5%
                    layer.synapses[neuronIdx][weightIdx] *= 1.03; // Renforcement synaptique 3%
                }
                
                // Adaptation du facteur d'adaptation
                neuron.adaptationFactor = qMin(1.5, neuron.adaptationFactor * 1.02);
            }
        }
    }
}

void BiologicalNetwork::weakenFailurePaths(const LearningData &failureData)
{
    // Affaiblissement l√©ger des chemins ayant √©chou√©
    
    forwardPropagation(failureData.inputSignals);
    
    // Affaiblissement des connexions trop dominantes
    for (int layerIdx = 1; layerIdx < m_layers.size(); ++layerIdx) {
        NetworkLayer &layer = m_layers[layerIdx];
        
        for (int neuronIdx = 0; neuronIdx < layer.neurons.size(); ++neuronIdx) {
            BiologicalNeuron &neuron = layer.neurons[neuronIdx];
            
            if (neuron.activation > 0.8) { // Neurones sur-activ√©s
                // L√©ger affaiblissement
                for (int weightIdx = 0; weightIdx < neuron.weights.size(); ++weightIdx) {
                    neuron.weights[weightIdx] *= 0.98; // Affaiblissement 2%
                }
                
                // R√©duction du facteur d'adaptation
                neuron.adaptationFactor = qMax(0.5, static_cast<double>(neuron.adaptationFactor * 0.99));
            }
        }
    }
}

QVector<double> BiologicalNetwork::getNetworkOutput() const
{
    if (m_layers.isEmpty()) {
        return QVector<double>();
    }
    
    const NetworkLayer &outputLayer = m_layers.last();
    QVector<double> output(outputLayer.neurons.size());
    
    for (int i = 0; i < outputLayer.neurons.size(); ++i) {
        output[i] = outputLayer.neurons[i].activation;
    }
    
    return output;
}

void BiologicalNetwork::onMEASignalsReceived(const QVector<double> &signalData)
{
    QMutexLocker locker(&m_networkMutex);
    
    m_lastMEASignals = signalData;
    
    // Mise √† jour des neurones d'entr√©e avec les nouveaux signaux
    if (signalData.size() == m_config.neuronCount && !m_layers.isEmpty()) {
        for (int i = 0; i < signalData.size(); ++i) {
            m_layers[0].neurons[i].activation = signalData[i];
            m_layers[0].neurons[i].lastStimulation = QDateTime::currentMSecsSinceEpoch();
        }
    }
}

void BiologicalNetwork::onMiningResult(bool success, uint64_t nonce, int attempts)
{
    // Cr√©ation de donn√©es de contexte pour l'apprentissage
    LearningData contextData;
    contextData.inputSignals = m_lastMEASignals;
    contextData.targetNonce = nonce;
    contextData.wasSuccessful = success;
    contextData.attempts = attempts;
    contextData.computeTime = attempts / 1000.0; // Estimation
    contextData.timestamp = QDateTime::currentDateTime();
    
    // Ajout √† l'historique d'apprentissage
    addLearningExample(contextData);
    
    // Optimisation bas√©e sur le feedback
    optimizeFromFeedback(success, contextData);
}

void BiologicalNetwork::onOptimizationTimer()
{
    // Optimisation p√©riodique du r√©seau
    if (m_learningState == LearningState::Optimizing && m_learningHistory.size() > 20) {
        performRetroLearning();
    }
}

QString BiologicalNetwork::getNetworkDiagnostic() const
{
    QMutexLocker locker(&m_networkMutex);
    
    QString diagnostic;
    diagnostic += QString("=== DIAGNOSTIC R√âSEAU BIOLOGIQUE ===\n");
    diagnostic += QString("√âtat: %1\n").arg(static_cast<int>(m_learningState));
    diagnostic += QString("Efficacit√©: %1%\n").arg(m_networkEfficiency * 100, 0, 'f', 1);
    diagnostic += QString("√âpoque courante: %1/%2\n").arg(m_currentEpoch).arg(m_totalEpochs);
    diagnostic += QString("Pr√©dictions r√©ussies: %1/%2 (%3%)\n")
                    .arg(m_successfulPredictions)
                    .arg(m_totalPredictions)
                    .arg(m_totalPredictions > 0 ? m_successfulPredictions * 100.0 / m_totalPredictions : 0.0, 0, 'f', 1);
    diagnostic += QString("Confiance moyenne: %1%\n").arg(m_averageConfidence * 100, 0, 'f', 1);
    diagnostic += QString("Exemples d'apprentissage: %1\n").arg(m_learningHistory.size());
    diagnostic += QString("Patterns m√©moris√©s: %1\n").arg(m_patternMemory.size());
    
    // D√©tails des couches
    diagnostic += QString("\n=== ARCHITECTURE ===\n");
    for (int i = 0; i < m_layers.size(); ++i) {
        const NetworkLayer &layer = m_layers[i];
        diagnostic += QString("Couche %1 (%2): %3 neurones, activation: %4\n")
                        .arg(i)
                        .arg(layer.layerType)
                        .arg(layer.neurons.size())
                        .arg(layer.layerActivation, 0, 'f', 3);
    }
    
    return diagnostic;
}

QVector<double> BiologicalNetwork::getLayerActivations(int layer) const
{
    QMutexLocker locker(&m_networkMutex);
    
    if (layer < 0 || layer >= m_layers.size()) {
        return QVector<double>();
    }
    
    const NetworkLayer &targetLayer = m_layers[layer];
    QVector<double> activations(targetLayer.neurons.size());
    
    for (int i = 0; i < targetLayer.neurons.size(); ++i) {
        activations[i] = targetLayer.neurons[i].activation;
    }
    
    return activations;
}

double BiologicalNetwork::getNetworkEfficiency() const
{
    QMutexLocker locker(&m_networkMutex);
    return m_networkEfficiency;
}

bool BiologicalNetwork::saveNetwork(const QString &filepath)
{
    QString path = filepath;
    if (path.isEmpty()) {
        QString dataDir = QStandardPaths::writableLocation(QStandardPaths::AppDataLocation);
        QDir().mkpath(dataDir);
        path = dataDir + "/biological_network.json";
    }
    
    QJsonObject networkState = exportNetworkState();
    QJsonDocument doc(networkState);
    
    QFile file(path);
    if (!file.open(QIODevice::WriteOnly)) {
        qWarning() << "[BIO-NET] Impossible de sauvegarder le r√©seau:" << path;
        return false;
    }
    
    file.write(doc.toJson());
    
    qDebug() << "[BIO-NET] R√©seau sauvegard√©:" << path;
    return true;
}

bool BiologicalNetwork::loadNetwork(const QString &filepath)
{
    QString path = filepath;
    if (path.isEmpty()) {
        QString dataDir = QStandardPaths::writableLocation(QStandardPaths::AppDataLocation);
        path = dataDir + "/biological_network.json";
    }
    
    QFile file(path);
    if (!file.open(QIODevice::ReadOnly)) {
        qWarning() << "[BIO-NET] Impossible de charger le r√©seau:" << path;
        return false;
    }
    
    QJsonParseError error;
    QJsonDocument doc = QJsonDocument::fromJson(file.readAll(), &error);
    
    if (error.error != QJsonParseError::NoError) {
        qWarning() << "[BIO-NET] Erreur parsing JSON:" << error.errorString();
        return false;
    }
    
    bool success = importNetworkState(doc.object());
    
    if (success) {
        qDebug() << "[BIO-NET] R√©seau charg√©:" << path;
    }
    
    return success;
}

QJsonObject BiologicalNetwork::exportNetworkState() const
{
    QMutexLocker locker(&m_networkMutex);

    QJsonObject state;

    // Configuration
    QJsonObject config;
    config["neuronCount"] = m_config.neuronCount;

    // S√©rialise hiddenLayers comme QJsonArray
    QJsonArray hiddenLayersArray;
    for (int val : m_config.hiddenLayers)
        hiddenLayersArray.append(val);
    config["hiddenLayers"] = hiddenLayersArray;

    config["learningRate"] = m_config.learningRate;
    config["stimulationThreshold"] = m_config.stimulationThreshold;
    config["adaptationRate"] = m_config.adaptationRate;
    config["memoryDepth"] = m_config.memoryDepth;
    config["useReinforcementLearning"] = m_config.useReinforcementLearning;
    // Ajoute d'autres param√®tres si besoin
    state["config"] = config;

    // √âtat du r√©seau
    QJsonObject networkState;
    networkState["learningState"] = static_cast<int>(m_learningState);
    networkState["currentEpoch"] = m_currentEpoch;
    networkState["totalEpochs"] = m_totalEpochs;
    networkState["trainingProgress"] = m_trainingProgress;
    networkState["networkEfficiency"] = m_networkEfficiency;
    networkState["successfulPredictions"] = m_successfulPredictions;
    networkState["totalPredictions"] = m_totalPredictions;
    networkState["averageConfidence"] = m_averageConfidence;
    state["networkState"] = networkState;

    // Couches du r√©seau (structure simplifi√©e)
    QJsonArray layers;
    for (const NetworkLayer &layer : m_layers) {
        QJsonObject layerObj;
        layerObj["type"] = layer.layerType;
        layerObj["neuronCount"] = layer.neurons.size();
        layerObj["layerActivation"] = layer.layerActivation;

        // Seuils des neurones
        QJsonArray thresholds;
        for (const BiologicalNeuron &neuron : layer.neurons) {
            thresholds.append(neuron.threshold);
        }
        layerObj["thresholds"] = thresholds;

        layers.append(layerObj);
    }
    state["layers"] = layers;

    // Timestamp et version
    state["timestamp"] = QDateTime::currentDateTime().toString(Qt::ISODate);
    state["version"] = "1.0.0";

    return state;
}

bool BiologicalNetwork::importNetworkState(const QJsonObject &state)
{
    QMutexLocker locker(&m_networkMutex);

    try {
        // Configuration
        if (state.contains("config")) {
            QJsonObject config = state["config"].toObject();
            m_config.neuronCount = config["neuronCount"].toInt();

            // D√©s√©rialise hiddenLayers depuis QJsonArray
            m_config.hiddenLayers.clear();
            if (config.contains("hiddenLayers") && config["hiddenLayers"].isArray()) {
                QJsonArray arr = config["hiddenLayers"].toArray();
                for (const auto &v : arr)
                    m_config.hiddenLayers.append(v.toInt());
            }

            m_config.learningRate = config["learningRate"].toDouble();
            m_config.stimulationThreshold = config["stimulationThreshold"].toDouble();
            m_config.adaptationRate = config["adaptationRate"].toDouble();
            m_config.memoryDepth = config["memoryDepth"].toInt();
            m_config.useReinforcementLearning = config["useReinforcementLearning"].toBool();
            // Ajoute d'autres param√®tres si besoin
        }

        // √âtat du r√©seau
        if (state.contains("networkState")) {
            QJsonObject networkState = state["networkState"].toObject();
            m_learningState = static_cast<LearningState>(networkState["learningState"].toInt());
            m_currentEpoch = networkState["currentEpoch"].toInt();
            m_totalEpochs = networkState["totalEpochs"].toInt();
            m_trainingProgress = networkState["trainingProgress"].toDouble();
            m_networkEfficiency = networkState["networkEfficiency"].toDouble();
            m_successfulPredictions = networkState["successfulPredictions"].toInt();
            m_totalPredictions = networkState["totalPredictions"].toInt();
            m_averageConfidence = networkState["averageConfidence"].toDouble();
        }

        // (Optionnel) D√©s√©rialisation des couches etc.

        return true;
    } catch (...) {
        qWarning() << "[BIO-NET] Erreur lors de l'importation de l'√©tat du r√©seau";
        return false;
    }
}


double BiologicalNetwork::getNetworkComplexity() const
{
    // Placeholder implementation for network complexity
    // In a real scenario, this would involve a more complex calculation
    // based on the network's structure, number of active neurons, 
    // and diversity of synaptic weights.
    return 0.5; 
}

bool BiologicalNetwork::initialize()
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::initialize() called.";
    initializeNetwork(); // Re-initialize or ensure network is set up
    return true;
}

bool BiologicalNetwork::configureNetwork(const NetworkConfig &config)
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::configureNetwork() called.";
    setNetworkConfig(config);
    return true;
}

bool BiologicalNetwork::initializeLearning(const NetworkConfig &config)
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::initializeLearning() called.";
    m_config = config; // Update config for learning
    return startInitialLearning(m_config.maxEpochs);
}

void BiologicalNetwork::setAdaptiveLearning(bool enable)
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::setAdaptiveLearning() called with" << enable;
    m_config.enableAdaptation = enable;
    // Further logic to enable/disable adaptive learning features
}

void BiologicalNetwork::updateInputSignals(const QVector<double> &currentSignals)
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::updateInputSignals() called.";
    onMEASignalsReceived(currentSignals); // Delegate to existing slot
}

void BiologicalNetwork::updateWeights()
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::updateWeights() called.";
    // This would typically involve a learning cycle, for now, a simple adjustment.
    adjustSynapticWeights(m_config.learningRate / 10.0); // Small adjustment
}

QVector<double> BiologicalNetwork::getOutputValues()
{
    QMutexLocker locker(&m_networkMutex);
    qDebug() << "[BIO-NET] BiologicalNetwork::getOutputValues() called.";
    return getNetworkOutput();
}

// ===================================================================
// NOUVELLES M√âTHODES BITCOIN MINING - G√âN√âRATION DE TRAINING DATA
// ===================================================================

/**
 * @brief G√©n√®re un exemple d'entra√Ænement Bitcoin r√©aliste
 */
BiologicalNetwork::LearningData BiologicalNetwork::generateBitcoinTrainingExample(
    int difficultyLevel, int exampleIndex)
{
    LearningData example;
    
    // === 1. G√âN√âRATION DU BLOCK HEADER R√âALISTE ===
    BitcoinBlockHeader blockHeader = generateRealisticBlockHeader(exampleIndex);
    
    // === 2. RECHERCHE D'UN NONCE VALIDE ===
    QPair<uint64_t, QString> solution = findValidNonce(blockHeader, difficultyLevel);
    uint64_t validNonce = solution.first;
    QString winningHash = solution.second;
    
    // === 3. CONVERSION EN SIGNAUX MEA BIOLOGIQUES ===
    example.inputSignals = blockHeaderToMEASignals(blockHeader);
    
    // === 4. CONFIGURATION DE LA CIBLE D'APPRENTISSAGE ===
    example.targetNonce = validNonce;
    example.blockHeader = blockHeaderToString(blockHeader);
    example.difficulty = calculateDifficultyBits(difficultyLevel);
    example.wasSuccessful = true; // Tous les exemples d'entra√Ænement sont des succ√®s
    
    // === 5. M√âTRIQUES D'APPRENTISSAGE ===
    example.attempts = estimateAttemptsForDifficulty(difficultyLevel);
    example.computeTime = example.attempts / 1000000.0; // Simulation r√©aliste
    example.timestamp = QDateTime::currentDateTime().addSecs(-exampleIndex);
    
    return example;
}

/**
 * @brief G√©n√®re un block header Bitcoin r√©aliste
 */
BiologicalNetwork::BitcoinBlockHeader BiologicalNetwork::generateRealisticBlockHeader(int seed)
{
    BitcoinBlockHeader header;
    
    // Version Bitcoin r√©aliste
    header.version = 0x20000000; // Version courante
    
    // Hash du block pr√©c√©dent (simulation r√©aliste)
    QCryptographicHash prevHash(QCryptographicHash::Sha256);
    prevHash.addData(QString("previous_block_%1_%2")
                     .arg(seed)
                     .arg(QDateTime::currentMSecsSinceEpoch())
                     .toLatin1());
    header.previousBlockHash = prevHash.result().toHex();
    
    // Merkle root (simulation de transactions)
    QCryptographicHash merkleHash(QCryptographicHash::Sha256);
    for (int tx = 0; tx < (seed % 10) + 1; ++tx) {
        merkleHash.addData(QString("transaction_%1_%2").arg(seed).arg(tx).toLatin1());
    }
    header.merkleRoot = merkleHash.result().toHex();
    
    // Timestamp r√©aliste (variation ¬±2 heures)
    header.timestamp = QDateTime::currentSecsSinceEpoch() + (seed % 7200) - 3600;
    header.difficultyBits = 0x1d00ffff; // Valeur par d√©faut, sera mise √† jour
    
    // Nonce sera d√©termin√© par l'algorithme de recherche
    header.nonce = 0;
    
    return header;
}

/**
 * @brief Trouve un nonce valide pour le block header donn√©
 */
QPair<uint64_t, QString> BiologicalNetwork::findValidNonce(
    const BitcoinBlockHeader &header, int difficultyLevel)
{
    QString targetPattern = QString("0").repeated(difficultyLevel);
    
    // Recherche efficace avec saut adaptatif
    //uint64_t maxAttempts = qMin(10000000ULL, 1ULL << (difficultyLevel * 4));
    //uint64_t stepSize = qMax(1ULL, maxAttempts / 100000);
    uint64_t maxAttempts = std::min(10000000ULL, 1ULL << (difficultyLevel * 4));
    uint64_t stepSize = std::max(uint64_t(1), uint64_t(maxAttempts / 100000));
    
    for (uint64_t nonce = 0; nonce < maxAttempts; nonce += stepSize) {
        BitcoinBlockHeader testHeader = header;
        testHeader.nonce = static_cast<uint32_t>(nonce);
        
        QString blockHash = calculateSHA256DoubleHash(testHeader);
        
        if (blockHash.startsWith(targetPattern)) {
            qDebug() << "[BIO-NET] üéØ Nonce trouv√©:" << QString("0x%1").arg(nonce, 8, 16, QChar('0'))
                     << "Hash:" << blockHash.left(16) + "...";
            return qMakePair(nonce, blockHash);
        }
    }
    
    // Fallback : g√©n√©ration artificielle garantie
    return generateArtificialSolution(header, difficultyLevel);
}

/**
 * @brief G√©n√®re une solution artificielle si aucune n'est trouv√©e rapidement
 */
QPair<uint64_t, QString> BiologicalNetwork::generateArtificialSolution(
    const BitcoinBlockHeader &header, int difficultyLevel)
{
    // G√©n√©ration d'un hash artificiel avec le bon nombre de z√©ros
    QString artificialHash = QString("0").repeated(difficultyLevel);
    
    // Compl√©ter avec des caract√®res hex al√©atoires
    QString hexChars = "0123456789abcdef";
    for (int i = difficultyLevel; i < 64; ++i) {
        artificialHash += hexChars[QRandomGenerator::global()->bounded(16)];
    }
    
    // Nonce artificiel bas√© sur les donn√©es du header
    uint64_t artificialNonce = qHash(header.previousBlockHash + header.merkleRoot) % (1ULL << 32);
    
    qDebug() << "[BIO-NET] üîß Solution artificielle g√©n√©r√©e pour difficult√©" << difficultyLevel;
    
    return qMakePair(artificialNonce, artificialHash);
}

/**
 * @brief Convertit un block header en signaux MEA biologiquement r√©alistes
 */
QVector<double> BiologicalNetwork::blockHeaderToMEASignals(const BitcoinBlockHeader &header)
{
    QVector<double> inputs(m_config.neuronCount);
    
    // S√©rialisation du block header
    QByteArray headerBytes = serializeBlockHeader(header);
    
    // === CONVERSION EN PATTERNS BIOLOGIQUES ===
    for (int electrode = 0; electrode < m_config.neuronCount; ++electrode) {
        double signal = 0.0;
        
        // 1. Composante de base d√©riv√©e des donn√©es Bitcoin
        int byteIndex = electrode % headerBytes.size();
        uint8_t headerByte = static_cast<uint8_t>(headerBytes[byteIndex]);
        
        // Amplitude de base (0.0 - 1.0)
        double baseAmplitude = headerByte / 255.0;
        
        // 2. Fr√©quence caract√©ristique bas√©e sur la position
        double frequency = 0.1 + (electrode * 0.02); // 0.1 - 2.3 Hz
        
        // 3. Pattern temporel Bitcoin-sp√©cifique
        double bitcoinPhase = (header.timestamp * 0.001) + (electrode * 0.1);
        double temporalPattern = sin(bitcoinPhase * frequency);
        
        // 4. Modulation par les bits significatifs
        int bitCount = qPopulationCount(headerByte); // Nombre de bits √† 1
        double bitDensityModulation = bitCount / 8.0;
        
        // 5. Composante de corr√©lation inter-√©lectrodes
        double correlationComponent = 0.0;
        if (electrode > 0) {
            correlationComponent = inputs[electrode - 1] * 0.15; // Couplage faible
        }
        
        // 6. Bruit biologique r√©aliste
        double biologicalNoise = (QRandomGenerator::global()->generateDouble() - 0.5) * 0.05;
        
        // === ASSEMBLAGE FINAL ===
        signal = baseAmplitude * (0.6 + temporalPattern * 0.3 + bitDensityModulation * 0.1)
                + correlationComponent + biologicalNoise;
        
        // Contraintes biologiques r√©alistes
        inputs[electrode] = qBound(-2.0, signal, 2.0);
    }
    
    // === POST-TRAITEMENT BIOLOGIQUE ===
    applyBiologicalFiltering(inputs);
    
    return inputs;
}

/**
 * @brief Applique un filtrage biologique r√©aliste aux signaux
 */
void BiologicalNetwork::applyBiologicalFiltering(QVector<double> &inputs)
{
    // Filtrage passe-bas simple (simulation de la r√©ponse cellulaire)
    const double alpha = 0.3; // Constante de filtrage
    
    for (int i = 1; i < inputs.size(); ++i) {
        inputs[i] = alpha * inputs[i] + (1.0 - alpha) * inputs[i - 1];
    }
    
    // Normalisation douce pour √©viter la saturation
    double maxSignal = *std::max_element(inputs.begin(), inputs.end());
    double minSignal = *std::min_element(inputs.begin(), inputs.end());
    
    if (maxSignal - minSignal > 3.0) { // Plage trop large
        double scale = 3.0 / (maxSignal - minSignal);
        for (double &signal : inputs) {
            signal *= scale;
        }
    }
}

/**
 * @brief Calcule le hash SHA-256 double d'un block header
 */
QString BiologicalNetwork::calculateSHA256DoubleHash(const BitcoinBlockHeader &header)
{
    QByteArray headerBytes = serializeBlockHeader(header);
    
    // Premier hash SHA-256
    QCryptographicHash firstHash(QCryptographicHash::Sha256);
    firstHash.addData(headerBytes);
    QByteArray firstResult = firstHash.result();
    
    // Deuxi√®me hash SHA-256 (protocole Bitcoin)
    QCryptographicHash secondHash(QCryptographicHash::Sha256);
    secondHash.addData(firstResult);
    
    return secondHash.result().toHex();
}

/**
 * @brief S√©rialise un block header au format Bitcoin
 */
QByteArray BiologicalNetwork::serializeBlockHeader(const BitcoinBlockHeader &header)
{
    QByteArray serialized;
    QDataStream stream(&serialized, QIODevice::WriteOnly);
    stream.setByteOrder(QDataStream::LittleEndian);
    
    // Format Bitcoin standard
    stream << static_cast<quint32>(header.version);
    
    // Ajout des hashes (32 bytes chacun)
    QByteArray prevHashBytes = QByteArray::fromHex(header.previousBlockHash.toLatin1());
    serialized.append(prevHashBytes.left(32)); // S'assurer de 32 bytes
    
    QByteArray merkleBytes = QByteArray::fromHex(header.merkleRoot.toLatin1());
    serialized.append(merkleBytes.left(32)); // S'assurer de 32 bytes
    
    // Continuer avec les autres champs
    QDataStream stream2(&serialized, QIODevice::WriteOnly | QIODevice::Append);
    stream2.setByteOrder(QDataStream::LittleEndian);
    stream2 << static_cast<quint32>(header.timestamp);
    stream2 << static_cast<quint32>(header.difficultyBits);
    stream2 << static_cast<quint32>(header.nonce);
    
    return serialized;
}

/**
 * @brief Convertit un block header en string pour stockage
 */
QString BiologicalNetwork::blockHeaderToString(const BitcoinBlockHeader &header)
{
    return QString("BitcoinBlock{v:%1,prev:%2,merkle:%3,time:%4,diff:%5,nonce:%6}")
           .arg(header.version, 0, 16)
           .arg(header.previousBlockHash.left(16))
           .arg(header.merkleRoot.left(16))
           .arg(header.timestamp)
           .arg(header.difficultyBits, 0, 16)
           .arg(header.nonce);
}

/**
 * @brief G√©n√®re des patterns Bitcoin sp√©ciaux pour l'entra√Ænement
 */
void BiologicalNetwork::generateSpecialBitcoinPatterns(int count)
{
    qDebug() << "[BIO-NET] üåü G√©n√©ration de patterns Bitcoin sp√©ciaux...";
    
    for (int i = 0; i < count; ++i) {
        LearningData specialExample;
        
        // Patterns sp√©ciaux bas√©s sur des cas r√©els Bitcoin
        if (i % 4 == 0) {
            // Pattern "Genesis" (inspir√© du block Genesis)
            specialExample = generateGenesisLikePattern(i);
        } else if (i % 4 == 1) {
            // Pattern "Halving" (simulation de blocks de halving)
            specialExample = generateHalvingLikePattern(i);
        } else if (i % 4 == 2) {
            // Pattern "High difficulty" (tr√®s difficile)
            specialExample = generateHighDifficultyPattern(i);
        } else {
            // Pattern "Sequence" (nonces s√©quentiels)
            specialExample = generateSequentialPattern(i);
        }
        
        m_learningHistory.append(specialExample);
    }
}

/**
 * @brief G√©n√®re un pattern similaire au block Genesis
 */
BiologicalNetwork::LearningData BiologicalNetwork::generateGenesisLikePattern(int index)
{
    LearningData example;
    
    BitcoinBlockHeader header;
    header.version = 0x00000001; // Version Genesis
    header.previousBlockHash = QString("0").repeated(64); // Pas de block pr√©c√©dent
    header.merkleRoot = "4a5e1e4baab89f3a32518a88c31bc87f618f76673e2cc77ab2127b7afdeda33b";
    header.timestamp = 1231006505; // Timestamp Genesis approximatif
    header.difficultyBits = 0x1d00ffff;
    header.nonce = 2083236893; // Nonce Genesis approximatif
    
    example.inputSignals = blockHeaderToMEASignals(header);
    example.targetNonce = header.nonce;
    example.blockHeader = blockHeaderToString(header);
    example.difficulty = header.difficultyBits;
    example.wasSuccessful = true;
    example.attempts = 2083236893; // Nombre d'essais approximatif
    example.computeTime = example.attempts / 100000.0;
    example.timestamp = QDateTime::currentDateTime().addSecs(-index);
    
    return example;
}

/**
 * @brief G√©n√®re un pattern de type halving
 */
BiologicalNetwork::LearningData BiologicalNetwork::generateHalvingLikePattern(int index)
{
    // Pattern bas√© sur des blocks de halving historiques
    return generateBitcoinTrainingExample(3, 210000 + index); // Difficult√© √©lev√©e
}

/**
 * @brief G√©n√®re un pattern √† haute difficult√©
 */
BiologicalNetwork::LearningData BiologicalNetwork::generateHighDifficultyPattern(int index)
{
    return generateBitcoinTrainingExample(4, 700000 + index); // Difficult√© maximale
}

/**
 * @brief G√©n√®re un pattern avec nonces s√©quentiels
 */
BiologicalNetwork::LearningData BiologicalNetwork::generateSequentialPattern(int index)
{
    LearningData example = generateBitcoinTrainingExample(2, index);
    
    // Modifier le nonce pour cr√©er une s√©quence
    example.targetNonce = (example.targetNonce & 0xFFFF0000) | (index % 65536);
    
    return example;
}

/**
 * @brief Calcule les bits de difficult√© pour un nombre de z√©ros requis
 */
uint64_t BiologicalNetwork::calculateDifficultyBits(int requiredZeros)
{
    // Formule Bitcoin simplifi√©e pour l'entra√Ænement
    return 0x1d000000 + (0x00ffffff >> requiredZeros);
}

/**
 * @brief Estime le nombre de tentatives n√©cessaires pour une difficult√©
 */
int BiologicalNetwork::estimateAttemptsForDifficulty(int difficultyLevel)
{
    // Estimation statistique bas√©e sur la probabilit√©
    int baseAttempts = 1000;
    return baseAttempts * static_cast<int>(std::pow(16, difficultyLevel));
}

/**
 * @brief M√©lange les donn√©es d'entra√Ænement pour √©viter l'overfitting
 */
void BiologicalNetwork::shuffleTrainingData()
{
    // Algorithme de m√©lange Fisher-Yates
    for (int i = m_learningHistory.size() - 1; i > 0; --i) {
        int j = QRandomGenerator::global()->bounded(i + 1);
        m_learningHistory.swapItemsAt(i, j);
    }
}


//#include "biological_network.moc"
